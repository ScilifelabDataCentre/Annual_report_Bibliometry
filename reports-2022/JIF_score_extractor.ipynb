{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JIF Score Extractor\n",
    "\n",
    "This notebook helps to retrieve JIF scores by ISSN codes.\n",
    "\n",
    "It is a semi-automatic process with a few manual steps.\n",
    "\n",
    "Preperations:\n",
    "- Create a Python virtual environment as descibed in the repository README Setup instructions.\n",
    "- Select the Python kernel from the virtual environment (venv/bin/python)\n",
    "- If needed, create sub-folder \"data/data2022/pub_extracts\"\n",
    "- If needed, create sub-folder \"data/data2022/temp\"\n",
    "\n",
    "To use:\n",
    "- Copy excel files containing ISSN codes into sub-folder \"data/data2022/pub_extracts\"\n",
    "- Edit the settings as needed\n",
    "- Run each cell\n",
    "- When you reach the cell with instructions about fetching JCR codes, perform the manual steps.\n",
    "- Finish by running the remaining cells\n",
    "\n",
    "Generated artifacts during the process include:\n",
    "- a set of CSV files containing ISSN codes in sub-folder \"JIF_ISSN_lists\"\n",
    "- a final Excel file containing JIF scores named \"JIF_Scores_[Year]_[date].xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before executing, select the python kernel from the local virtual env\n",
    "# Requires the ipykernel is installed\n",
    "\n",
    "print(\"Verify can run jupyter cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "MAX_ISSN_CODES_PER_CSV_FILE = 600     # The maximum number of ISSN codes to write to each CSV file.\n",
    "DATA_PATH = \"../data/data2022\"        # The relative path to the input Excel files containing the ISSN codes.\n",
    "INPUT_COL_NAMES = [\"ISSN\", \"ISSN-L\"]  # A list of column names to process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "print(\"Finished imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile list of ISSN codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(DATA_PATH, \"pub_extracts\")\n",
    "\n",
    "print(f\"Searching for input files in path: {input_path}\")\n",
    "\n",
    "if not os.path.isdir(input_path):\n",
    "    raise Exception(f\"Path does not exists: {input_path}\")\n",
    "\n",
    "\n",
    "vals_list = []\n",
    "\n",
    "for filename in os.listdir(input_path):\n",
    "    f = os.path.join(input_path, filename)\n",
    "    if os.path.isfile(f):\n",
    "        print(f)\n",
    "\n",
    "        df = pd.read_excel(\n",
    "            f,\n",
    "            sheet_name=0, # first sheet\n",
    "            header=0,\n",
    "            engine=\"openpyxl\",\n",
    "            keep_default_na=False,\n",
    "            usecols=INPUT_COL_NAMES,\n",
    "            dtype=str\n",
    "        )\n",
    "\n",
    "        for col in df:\n",
    "            vals_list = vals_list + df[col].tolist()\n",
    "\n",
    "\n",
    "print(f\"Total nr of ISSN codes before cleaning: {len(vals_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the list of ISSN codes\n",
    "\n",
    "issn_x_df = pd.DataFrame(vals_list, columns=['ISSN']).apply(lambda x: x.astype(str).str.upper())\n",
    "\n",
    "print(issn_x_df.shape)\n",
    "\n",
    "\n",
    "# Remove blank rows\n",
    "\n",
    "issn_x_df[\"ISSN\"] = issn_x_df[\"ISSN\"].str.strip()\n",
    "\n",
    "issn_x_df.replace('', np.nan, inplace=True)\n",
    "issn_x_df.dropna(inplace=True)\n",
    "\n",
    "print(issn_x_df.shape)\n",
    "\n",
    "\n",
    "# Sort by ISSN\n",
    "\n",
    "issn_x_df.sort_values(by=[\"ISSN\"], inplace=True)\n",
    "\n",
    "\n",
    "# Remove duplicate values\n",
    "issn_x_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(issn_x_df.shape)\n",
    "\n",
    "\n",
    "# Split into ISSN and ISBN codes\n",
    "\n",
    "isbn_idx = issn_x_df['ISSN'].str.startswith('ISBN')\n",
    "isbn_df = issn_x_df[isbn_idx]\n",
    "issn_df = issn_x_df[~isbn_idx]\n",
    "\n",
    "print(f\"Nr of ISSN's: {len(issn_df)}\")\n",
    "print(f\"Nr of ISBN's: {len(isbn_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv files\n",
    "\n",
    "For use to extract from Clarivate.\n",
    "\n",
    "The ISSN files will have names in format \"issn_list_[year]_file[file number].csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv files\n",
    "\n",
    "year = datetime.today().year\n",
    "\n",
    "\n",
    "# Create output folder if not exists\n",
    "\n",
    "output_path = os.path.join(DATA_PATH, \"JIF_ISSN_lists\")\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "\n",
    "# Save the ISBN file\n",
    "isbn_df.to_csv( os.path.join(output_path, f\"isbn_list_{year}.csv\"), index=False, header=False )\n",
    "\n",
    "# Save the ISSN files\n",
    "n_files = math.ceil(len(issn_df) / MAX_ISSN_CODES_PER_CSV_FILE)\n",
    "\n",
    "for file_nr in range(1, n_files+1):\n",
    "    filename = f\"issn_list_{year}_file{file_nr}.csv\"\n",
    "    startidx = (file_nr-1) * MAX_ISSN_CODES_PER_CSV_FILE\n",
    "    endidx = (file_nr*MAX_ISSN_CODES_PER_CSV_FILE)\n",
    "    print(f\"Now saving file {filename} with rows for index between {startidx} and {endidx}\")\n",
    "    issn_df.iloc[startidx:endidx].to_csv( os.path.join(output_path, filename), index=False, header=False )\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUAL STEPS: Retrieve JCR codes\n",
    "\n",
    "- In Clarivate, select the desired columns.\n",
    "- For each CSV file in /JIF_ISSN_list:\n",
    "    - Use the list of ISSN codes to filter journals by ISSN / eISSN code in Clarivate.\n",
    "    - Click Apply to activate the filter selection.\n",
    "    - Click export. Choose XLS (excel)\n",
    "    - Wait for the download.\n",
    "- Move the downloaded files to the /temp folder in this project.\n",
    "\n",
    "When all files are extracted and moved to /temp, continue with the below steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify to clean the partial extracts\n",
    "\n",
    "temp_path = os.path.join(DATA_PATH, \"temp\")\n",
    "print(f\"Searching for partial, temporary files in path: {temp_path}\")\n",
    "\n",
    "if not os.path.isdir(temp_path):\n",
    "    raise Exception(f\"Path does not exists: {temp_path}\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(temp_path):\n",
    "    f = os.path.join(temp_path, filename)\n",
    "    if os.path.isfile(f):\n",
    "        print(f\"Now working on file {f}\")\n",
    "        rows_to_delete = []\n",
    "        wb = openpyxl.load_workbook(f)\n",
    "        sheet = wb.active\n",
    "        #print(f\"Nr of rows in excel sheet {sheet.max_row}\")\n",
    "\n",
    "        for row in sheet.iter_rows():\n",
    "\n",
    "            if (row[0].value == \"\" or row[0].value == None):\n",
    "                rowidx = row[0].row\n",
    "                rows_to_delete.append(rowidx)\n",
    "                #print(f\"Deleting an empty row {rowidx}.\")\n",
    "\n",
    "            elif (row[0].value.startswith(\"Journal Data Filtered\") or row[0].value.startswith(\"Copyright (c)\") or row[0].value.startswith(\"By exporting the selected data\")):\n",
    "                rowidx = row[0].row\n",
    "                rows_to_delete.append(rowidx)\n",
    "                #print(f\"Deleting row {rowidx} with beginning text: {row[0].value, row[1].value}\" )\n",
    "\n",
    "        #print(f\"Now deleting rows with indexes {rows_to_delete}\")\n",
    "        for i in reversed(rows_to_delete):\n",
    "            sheet.delete_rows(i, 1)\n",
    "\n",
    "        # Saving to a new file with prefix \"cleaned_\"\n",
    "        \n",
    "        new_file_path = os.path.join(temp_path, \"cleaned_\"+filename)\n",
    "        sheet.title = \"JCR\"\n",
    "        wb.save(new_file_path)\n",
    "        print(f\"Done editing excel file {f}. Saved to {new_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine into one JCR scores output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the partial extracts into one complete JIF file\n",
    "# Save the resulting cleaned output file to /JIF_scores/JCR_JournalResults_[year]_byISSN_[date].xlsx\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "# Create output folder if not exists\n",
    "\n",
    "output_path = os.path.join(DATA_PATH, \"JIF_scores\")\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "\n",
    "# Output filename\n",
    "\n",
    "today = datetime.today().date()\n",
    "year = today.year\n",
    "output_filename = f\"JCR_JournalResults_{year}_byISSN_run{today.strftime('%Y%m%d')}.xlsx\"\n",
    "output_path = os.path.join(DATA_PATH, \"JIF_scores\", output_filename)\n",
    "\n",
    "\n",
    "# Safest to delete existing file if it exists\n",
    "\n",
    "if os.path.isfile(output_path):\n",
    "     os.remove(output_path)\n",
    "\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "temp_path = os.path.join(DATA_PATH, \"temp\")\n",
    "print(f\"Searching for cleaned, temporary files in path: {temp_path}\")\n",
    "\n",
    "filenames = glob.glob(temp_path + \"/cleaned_*.xlsx\")\n",
    "\n",
    "\n",
    "for file in filenames:\n",
    "   df = pd.concat(pd.read_excel( file, sheet_name=None), ignore_index=True, sort=False)\n",
    "\n",
    "   output_df = output_df.append( df, ignore_index=True)\n",
    "\n",
    "output_df.to_excel(output_path, index=False, sheet_name=\"JCR\")\n",
    "print(f\"Saved final excel file to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ef58c07f7a5dca0b590f005ceb097b33b049cd2b2ed194ecb2d4e898b3ec7c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
